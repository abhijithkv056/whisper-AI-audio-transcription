{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.callbacks import CallbackManager\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "\n",
    "PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"\"You are a Call transcription agent. \n",
    "        You are given 911 call transcript and you need to extract the information from the transcript.\n",
    "        You need to table the information in a structured format in the form of a chatbot.\n",
    "        The information should be presented in following format:\n",
    "        Caller chat: \n",
    "        911 call chat:\n",
    "        Caller chat:\n",
    "        911 operator chat:\n",
    "        ...\n",
    "        \n",
    "        \"\"\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = \"Hello? This is the Wachovia Bank. It's 1302 West International Speedway Boulevard. Okay. We're being robbed. Okay. Where are you at in the bank? He's saying he has a bomb. He's saying he has a taser. Okay. White male, black male? White male, black male. White male. White male. Okay. Black beanie, black hat. \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caller chat: \n",
      "Hello? This is the Wachovia Bank. It's 1302 West International Speedway Boulevard. \n",
      "\n",
      "911 call chat:\n",
      "Okay. We're being robbed. \n",
      "\n",
      "Caller chat:\n",
      "He's saying he has a bomb. He's saying he has a taser. \n",
      "\n",
      "911 operator chat:\n",
      "Okay. Where are you at in the bank? \n",
      "\n",
      "Caller chat:\n",
      "White male, black male? \n",
      "\n",
      "911 operator chat:\n",
      "White male, black male. \n",
      "\n",
      "Caller chat:\n",
      "White male. \n",
      "\n",
      "911 operator chat:\n",
      "Okay. \n",
      "\n",
      "Caller chat:\n",
      "Black beanie, black hat.\n"
     ]
    }
   ],
   "source": [
    "outputparser = StrOutputParser()\n",
    "\n",
    "chain = PROMPT | llm | outputparser\n",
    "\n",
    "result = chain.invoke(transcription)\n",
    "\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.1\")\n",
    "\n",
    "chain = PROMPT | llm | outputparser\n",
    "\n",
    "result = chain.invoke(transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the extracted information from the transcript:\n",
      "\n",
      "**Caller chat:**\n",
      "* I'm at Wachovia Bank, located at 1302 West International Speedway Boulevard.\n",
      "* We are being robbed.\n",
      "\n",
      "**911 call chat:**\n",
      "* The suspect has a bomb.\n",
      "* The suspect also has a taser.\n",
      "* Description of suspects:\n",
      "\t+ Suspect 1: White male\n",
      "\t+ Suspect 2: Black male (later confirmed as wearing black beanie and black hat)\n",
      "\n",
      "Let me know if you'd like me to add anything else!\n"
     ]
    }
   ],
   "source": [
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Any', 'AsyncCallbackManagerForLLMRun', 'AsyncClient', 'AsyncIterator', 'BaseLLM', 'CallbackManagerForLLMRun', 'Client', 'Dict', 'GenerationChunk', 'Iterator', 'LLMResult', 'LangSmithParams', 'List', 'Literal', 'Mapping', 'OllamaLLM', 'Optional', 'Options', 'PrivateAttr', 'Self', 'Union', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'model_validator']\n"
     ]
    }
   ],
   "source": [
    "import langchain_ollama.llms\n",
    "print(dir(langchain_ollama.llms))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
